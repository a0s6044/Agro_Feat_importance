{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Input vector construction and Harvest prediction for all Heddingle fields from data in a single year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "id": "AcI6T3Xowumi"
   },
   "outputs": [],
   "source": [
    "# In particular we now include the correct dates for weather grouping for hostvete \n",
    "# which begin from seed date -> 1st Nov year before + 15Marh->midsummer + midsummer -> max harvest date\n",
    "\n",
    "# We load all soil, field, year and weather data for all fields in Heddinge and predict \n",
    "# the harvest for that field during a single year: 2017, 2018, 2019 or 2020\n",
    "\n",
    "# Also change the grouping in time for weather data to just a single season (Aug 2)\n",
    "# instead of the previous weekly grouping\n",
    "\n",
    "# We segment per crop and year in this particular notebook (July 31, Aug 1)\n",
    "\n",
    "# First start the VPN in order to download all necessary data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "id": "gMxKmWmqwumy"
   },
   "outputs": [],
   "source": [
    "# Function to load a list with all properties from a particular field\n",
    "def loadfieldproperties(datain):\n",
    "    fieldattr=[]\n",
    "    fieldkeys=[]\n",
    "    for key in datain[\"properties\"].keys():\n",
    "        #print(singlefield[\"properties\"][key])\n",
    "        fieldattr.append(datain[\"properties\"][key])\n",
    "        fieldkeys.append(key)\n",
    "    return fieldattr, fieldkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "id": "MUoLuqW9wumz"
   },
   "outputs": [],
   "source": [
    "def keystolist(dictin):\n",
    "    x=[]\n",
    "    y=[]\n",
    "    for key in dictin.keys():\n",
    "        x.append(key)\n",
    "        y.append(dictin[key])\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "id": "gGH0NFE9wum1"
   },
   "outputs": [],
   "source": [
    "# loading a pickled (saved) list \n",
    "def loadfield(path_to_file, fieldID):\n",
    "  file_name = path_to_file+fieldID#+'.pkl'\n",
    "  open_file = open(file_name, \"rb\")\n",
    "  loaded_list = pickle.load(open_file)\n",
    "  open_file.close()\n",
    "\n",
    "  return loaded_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "id": "btp8r6tDCA8Y"
   },
   "outputs": [],
   "source": [
    "# Function to load a list with all properties from a particular field\n",
    "def loadfieldpropertiestoDF(datain):\n",
    "\n",
    "    fieldattr = []\n",
    "    fieldkeys = []\n",
    "    for key in datain[\"properties\"].keys():\n",
    "        fieldattr.append(datain[\"properties\"][key])\n",
    "        fieldkeys.append(key)\n",
    "\n",
    "    return pd.DataFrame([fieldattr], columns = fieldkeys) #tempDF #fieldattr, fieldkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "id": "FSIn-ihnCMA9"
   },
   "outputs": [],
   "source": [
    "# Loads all subfield data from a given field ID into lists\n",
    "def loadintodf(maindf, result):\n",
    "\n",
    "    subfieldIDs = len((result[\"SRData\"][0])[\"field\"])\n",
    "    #print(\"There are \"+str(subfieldIDs)+\" different fields within that field ID.\")\n",
    "    #print(\"These are: \")\n",
    "    #templist = []\n",
    "    for k in range(subfieldIDs):\n",
    "        print(k)\n",
    "        #print(\"ID: \",(result[\"SRData\"][0])[\"field\"][k][\"id\"])\n",
    "        #flist, fkeys = \n",
    "        tempdf = LoadFieldDataToDF(result, k) #loadfielddatatodf(result,k)\n",
    "        \n",
    "        #templist.append(flist)\n",
    "        maindf = maindf.append(tempdf)\n",
    "    #templist.insert(0, fkeys)\n",
    "    \n",
    "    return maindf # templist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "id": "5rRGRf7FYjG3"
   },
   "outputs": [],
   "source": [
    "# Creates a dataframe from a keylist and a datalist\n",
    "def ListstoDF(keylist, datalist):\n",
    "  return pd.DataFrame([datalist], columns=keylist)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "id": "_C1_Hxm3T-kb"
   },
   "outputs": [],
   "source": [
    "def ListfromData(indata, flag):\n",
    "\n",
    "    datalist = []\n",
    "    if flag == 0:\n",
    "        keyslist = ['id', 'coordinates']\n",
    "        datalist.append(indata['id']) # id data\n",
    "        coord = indata['geometry']['coordinates']\n",
    "        conv_data = coord # ConvUTM33NToWGS84(coord)\n",
    "        datalist.append(conv_data)       # geometry\n",
    "    elif flag  == 2: # For harvest\n",
    "        keyslist = ['id', 'coordinates']\n",
    "        datalist.append(indata['id']) # id data\n",
    "        coord = indata['geometry']['coordinates']\n",
    "        conv_data = coord # ConvUTM33NToWGS84(coord)\n",
    "        datalist.append(conv_data)       # geometry\n",
    "    else: # For weather\n",
    "        keyslist = []\n",
    "    \n",
    "    tempdata = indata['properties']  # properties\n",
    "    \n",
    "    templist = list(tempdata.keys())\n",
    "    for k in range(len(templist)):\n",
    "        keyslist.append(templist[k])\n",
    "        datalist.append(tempdata[templist[k]])\n",
    "    #keyslist.append(templist[-1])\n",
    "    #datalist.append(tempdata[templist[-1]])\n",
    " \n",
    "    return keyslist, datalist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "id": "mOLPfdFWakBG"
   },
   "outputs": [],
   "source": [
    "def loadfieldtoDF(tempdata, flag):\n",
    "  #tempdata = allvalues[0]\n",
    "  #print(tempdata)\n",
    "  finalDF = pd.DataFrame()\n",
    "  for k in tqdm(range(len(tempdata))): # each tempdata is a single field containing a number of time soil data\n",
    "    #print(tempdata[k])\n",
    "    #print(\"Progress : \",k,'/',len(tempdata))\n",
    "    keylist, datalist =  ListfromData(tempdata[k], flag)\n",
    "    tempDF  = ListstoDF(keylist, datalist)\n",
    "    finalDF = finalDF.append(tempDF)\n",
    "  return finalDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "id": "BLAMgi8GYwnP"
   },
   "outputs": [],
   "source": [
    "# Converts dataframe to numpy array and saves it into binary file with name filename\n",
    "def svpdtonpy(arrayin, filename):\n",
    "  v = arrayin.reset_index()\n",
    "  w=np.rec.fromrecords(v, names=v.columns.tolist())\n",
    "  np.save(filename, w)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "id": "R3rRL-0LCI8g"
   },
   "outputs": [],
   "source": [
    "# Function to load all data from a specific field to a df\n",
    "# input fielddata of a response from a json request \n",
    "# input pickafield containing a specific field number \n",
    "def LoadFieldDataToDF(fielddata, pickafield):\n",
    "    \n",
    "    singlefield = (fielddata[\"SRData\"][0])[\"field\"][pickafield]\n",
    "    fieldcoord  = singlefield[\"geometry\"][\"coordinates\"]#[0] # coord for last field number \n",
    "    convcoord   = convwgs84ToSWEREF99(fieldcoord[0])# Convert field coordinates to latt, long\n",
    "    fieldprop = pd.DataFrame()\n",
    "    \n",
    "    fieldprop = loadfieldpropertiestoDF(singlefield) # DF containing all properties of that particular field\n",
    "    fieldprop[\"geometry\"] = [convcoord]\n",
    "    fieldprop[\"id\"] = singlefield[\"id\"]\n",
    "    #fieldprop.append(convcoord)\n",
    "    #fieldprop.append(singlefield[\"id\"])\n",
    "    \n",
    "    #fieldkeys.append(\"geometry\")\n",
    "    #fieldkeys.append(\"id\")\n",
    "                  \n",
    "    return fieldprop #, fieldkeys # list containing all data from that particular field"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "Wr2B0NinTetA"
   },
   "outputs": [],
   "source": [
    "# load numpy array filename and convert to Pandas dataframe\n",
    "def loadnpy(filename):\n",
    "    temparray = np.load(filename, allow_pickle=True)\n",
    "    tempdf = pd.DataFrame(temparray)#, columns=temparray[0,1:])\n",
    "    return tempdf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# check if df has same values in whole column\n",
    "# Also remove any empty columns\n",
    "def clean_df(dfin):\n",
    "    for k in list(dfin.columns):\n",
    "        if dfin[k].isnull().all()==True:\n",
    "            dfin = dfin.drop(columns = [k])\n",
    "            print(\"Dropping empty column \", str(k))\n",
    "    return dfin \n",
    "\n",
    "# remove a column with a single value\n",
    "def remov(dfin, col):\n",
    "    firstvalue = dfin.iloc[0][col]\n",
    "    if dfin[col].all() == firstvalue:\n",
    "        print(\"All elements in column \", str(col), \"have the same value:\", firstvalue)\n",
    "        print(\"dropped column \", str(col))\n",
    "        dout = dfin.drop(columns=[col])\n",
    "    else:\n",
    "        print(\"There exist different values in column \", str(col), \". Keeping column intact.\")\n",
    "        dout = dfin\n",
    "    return dout\n",
    "\n",
    "# eliminates all data in provided column (not equal to given value)\n",
    "# Creates new dataframe with data from column based on given value\n",
    "def elim(dfin, col, value):\n",
    "    dout = dfin[dfin[col]==value]\n",
    "    print(\"kept only data which contain \", str(value), \" in column \", str(col))\n",
    "    return dout\n",
    "#list_of_dfs['w'][list_of_dfs['w']['year'] == 2017]# harv, soil, w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean weather dataframe \n",
    "def cleanDataFrame(dfin, cols_to_remove):\n",
    "    tempdf = clean_df(dfin)\n",
    "    dfout  = tempdf.drop(columns = cols_to_remove)#remove_columns)\n",
    "    return dfout\n",
    "\n",
    "# Convert date and time to just datetime object with only date\n",
    "#format_string = \"%Y-%m-%d\"\n",
    "#tempdf[\"time\"] = pd.to_datetime(tempdf[\"time\"], format = format_string)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setting up and initiating VPN "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def connect(params_dic):\n",
    "    \"\"\" Connect to the PostgreSQL database server \"\"\"\n",
    "    conn = None\n",
    "        \n",
    "    try:\n",
    "        # connect to the PostgreSQL server\n",
    "        print('Connecting to the PostgreSQL database...')\n",
    "        conn = psycopg2.connect(**params_dic)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(error)\n",
    "        sys.exit(1) \n",
    "    print(\"Connection successful\")\n",
    "    return conn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def postgresql_to_dataframe(conn, select_query, column_names):\n",
    "    \"\"\"\n",
    "    Tranform a SELECT query into a pandas dataframe\n",
    "    \"\"\"\n",
    "    cursor = conn.cursor()\n",
    "    try:\n",
    "        cursor.execute(select_query)\n",
    "    except (Exception, psycopg2.DatabaseError) as error:\n",
    "        print(\"Error: %s\" % error)\n",
    "        cursor.close()\n",
    "        return 1\n",
    "    \n",
    "    # Naturally we get a list of tupples\n",
    "    tupples = cursor.fetchall()\n",
    "    cursor.close()\n",
    "    \n",
    "    # We just need to turn it into a pandas dataframe\n",
    "    df = pd.DataFrame(tupples, columns=column_names)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calltoVPN(param_dic, textcom):\n",
    "    \n",
    "    # opening the VPN connection\n",
    "    dbConnection = connect(param_dic) # Connect to PostgreSQL server\n",
    "    \n",
    "    start_time = datetime.datetime.now()        \n",
    "    dfout = pd.read_sql_query(textcom, dbConnection)\n",
    "    dif = datetime.datetime.now()-start_time\n",
    "    print(\"Time to complete request:\", dif.seconds, \"sec.\")\n",
    "    \n",
    "    # Close the database connection\n",
    "    dbConnection.close();\n",
    "    print(\"Connection closed\")\n",
    "    \n",
    "    return dfout # output containing the response in a dataframe table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Call to obtain a list of farms (blockids) in Heddinge with given properties"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the different possible calls to be used later\n",
    "def chose_a_call(argument,  region_name='Heddinge', crop_name='Höstvete',fieldID = 123456789):\n",
    "    switcher = {\n",
    "        # Gets blockids from cultivation - Heddinge only\n",
    "        1: '''SELECT blockid FROM agri.cultivation WHERE farmname = 'Heddinge' ''',\n",
    "        \n",
    "        # Gets all from soil - specific field only\n",
    "        2: '''SELECT * FROM agri.soil_sample u,agri.cult_union c \\\n",
    "        WHERE c.blockid = '''+str(fieldID),\n",
    "        \n",
    "        # Gets all from cultivation - specific: Varvete, Heddinge and seed_date\n",
    "        3: '''SELECT * FROM agri.cultivation WHERE crop = 'Vårvete' \\\n",
    "        AND farmname = 'Heddinge' AND extract(month from seed_date) < 7''',\n",
    "        \n",
    "        # Gets all from cultivation - specific: Varvete, Heddinge and seed_date\n",
    "        4: '''SELECT * FROM agri.cultivation WHERE crop = 'Vårkorn' \\\n",
    "        AND farmname = 'Heddinge' AND extract(month from seed_date) < 7\\\n",
    "        AND yield_harvester_measured > 0 AND humus_average < 10 ORDER BY blockid ''',\n",
    "        \n",
    "        # Gets all from cultivation - specific: Varvete, Heddinge and seed_date\n",
    "        5: '''SELECT * FROM agri.cultivation WHERE crop = 'Höstvete' \\\n",
    "        AND farmname = 'Heddinge' AND extract(month from seed_date) < 7''',\n",
    "        \n",
    "        # unfinished call - do not use\n",
    "        6: '''agri.cultivation SET crop = 'Höstvete' \\\n",
    "        WHERE crop ILIKE '%vete%' AND extract(month from seed_date) > 6;''',\n",
    "        \n",
    "        # Gets all fields from cultivation - specific: yield, humus\n",
    "        7: '''SELECT * FROM agri.cultivation WHERE crop = 'Vårvete' \\\n",
    "        AND farmname = 'Heddinge' AND yield_harvester_measured > 0 AND humus_average < 10 ORDER BY blockid''',\n",
    "        \n",
    "        # This is latest command from Andreas about hostvete  #Vårkorn\n",
    "        8: '''SELECT blockid FROM agri.cultivation WHERE crop = 'Höstvete' \\\n",
    "        AND farmname = ''' + region_name +''' AND yield_harvester_measured > 2500 \\\n",
    "        AND blockid IS NOT NULL AND seed_date IS NOT NULL;''',\n",
    "        \n",
    "        # This is latest command from Andreas about varkorn\n",
    "        9: '''SELECT blockid, harvest_year FROM agri.cultivation WHERE crop = 'Vårkorn' \\\n",
    "        AND farmname = 'Heddinge' AND yield_harvester_measured > 2500 \\\n",
    "        AND blockid IS NOT NULL AND seed_date IS NOT NULL;''',\n",
    "    }\n",
    "    return switcher.get(argument,\"Invalid choise\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calls to database "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the different possible calls to be used later\n",
    "def new_calls(argument,  region_name = \"'Heddinge'\", crop_name=\"'Vårvete'\",fieldID = 123456789):\n",
    "    switcher = {\n",
    "        \n",
    "        # This is for weather \n",
    "        0: #\"Select * FROM agri.weather ORDER BY year ASC\",#w.temperature AS w_temperature, \\\n",
    "        \"SELECT w.precipitation AS w_precipitation, \\\n",
    "        w.year AS w_year, \\\n",
    "        w.time AS w_time \\\n",
    "        FROM agri.cultivation c,agri.weather w \\\n",
    "        WHERE c.blockid = \" + str(fieldID)+ \"\\\n",
    "        AND extract(month from w.time) < 11 \\\n",
    "        AND extract(month from w.time) > 2 \\\n",
    "        ORDER BY year ASC\",\n",
    "        \n",
    "        # This is for harvest\n",
    "        1:\n",
    "        \"SELECT ST_X(u.geom) AS h_coord_X, ST_Y(u.geom) AS h_coord_Y, \\\n",
    "        c.blockid AS h_blockid, \\\n",
    "        u.year AS h_year, \\\n",
    "        u.time AS h_time, \\\n",
    "        u.harvest AS h_harvest, \\\n",
    "        u.water_percentage AS h_water_percentage \\\n",
    "        FROM agri.harvest u, agri.cult_union c WHERE c.blockid = \"+str(fieldID)+\" \\\n",
    "        AND ST_Intersects(c.geom, u.geom) \\\n",
    "        AND u.crop = 'Vete' \\\n",
    "        AND u.edgeflag = 'False' \\\n",
    "        ORDER BY year,id ASC\",\n",
    "        #+ crop_name +\" \\\n",
    "        #\n",
    "        \n",
    "        # This is for soil\n",
    "        2: #'''SELECT ST_X(u.geom) AS coord_X, ST_Y(u.geom) AS coord_Y FROM agri.soil_sample u,agri.cult_union c WHERE c.blockid = '''+str(fieldID),\n",
    "        '''SELECT ST_X(u.geom) AS coord_X, ST_Y(u.geom) AS coord_Y, \\\n",
    "        u.ph AS s_ph, \\\n",
    "        u.humus_percentage AS s_hum100, \\\n",
    "        u.clay_percentage AS s_clay100, \\\n",
    "        u.phosphorus AS s_phosphorus, \\\n",
    "        u.calcium AS s_calcium, \\\n",
    "        u.potassium AS s_K, u.magnesium AS s_mg \\\n",
    "        FROM agri.soil_sample u,agri.cult_union c WHERE c.blockid = '''+str(fieldID)+''' \\\n",
    "        AND ST_IsValid(c.geom) \\\n",
    "        AND ST_Intersects(c.geom,u.geom) \\\n",
    "        AND u.youngest = 'True' ORDER BY year,id ASC''',\n",
    "        #\n",
    "        \n",
    "        \n",
    "        # This is for field\n",
    "        3:'''SELECT \\\n",
    "        blockid AS f_blockid,\\\n",
    "        seed_date AS f_seed_date, \\\n",
    "        seed_amount AS f_seed_amount, \\\n",
    "        harvest_date AS f_harvest_date, \\\n",
    "        yield_harvester_measured AS f_yield_harvester_measured, \\\n",
    "        yield_water_percentage AS f_yield_water_percentage, \\\n",
    "        fertilizer_date AS f_fertilizer_date, \\\n",
    "        fertilizer_amount AS f_fertilizer_amount, \\\n",
    "        humus_average AS f_humus_average, \\\n",
    "        humus_stddev AS f_humus_stddev,\\\n",
    "        ph_average AS f_ph_average, \\\n",
    "        ph_stddev AS f_ph_stddev, \\\n",
    "        weatherstation AS f_weatherstation, \\\n",
    "        weatherstation_dist AS f_weatherstation_dist, \\\n",
    "        blockid_org AS f_blockid_org, \\\n",
    "        nitrogen AS f_nitrogen, \\\n",
    "        kalium AS f_kalium, \\\n",
    "        phosphorus AS f_phosphorus, \\\n",
    "        sulphur AS f_sulphur, \\\n",
    "        calcium AS f_calcium, \\\n",
    "        magnesium AS f_MG, \\\n",
    "        nitrogen_earlier AS f_nitrogen_earlier, \\\n",
    "        edgeflag_percentage AS f_edgeflag_percentage \\\n",
    "        FROM agri.cultivation  \\\n",
    "        WHERE crop = ''' + crop_name +''' \\\n",
    "        AND farmname = 'Heddinge' \\\n",
    "        AND blockID = '''+str(fieldID),\n",
    "        # slopeness AS f_slopeness, \\\n",
    "        #nitrogen_secondary AS f_nitrogen_secondary, \\\n",
    "        #weather_precipitation AS f_weather_precip, \\\n",
    "        #weather_temperature AS f_weather_temp, \\\n",
    "        #harvest_year AS f_har_year, \\\n",
    "        #crop AS f_crop, \\\n",
    "        #geom AS f_geom, \\\n",
    "        #yield_amount AS f_yield_amount, \\\n",
    "        #''' + region_name +''' \\\n",
    "        # Gets all from soil_sample - specific: youngest, fieldID\n",
    "        \n",
    "        5:'''SELECT * FROM agri.soil_sample WHERE youngest = 'True' \\\n",
    "        AND c.blockid = '''+str(fieldID),\n",
    "        \n",
    "        # Unfinished call - do not use\n",
    "        10: \"SELECT a.id,a.year,b.id,b.year,ST_X(a.geom),ST_X(b.geom) \\\n",
    "        FROM agri.soil_sample a,agri.soil_sample b WHERE ST_DWithin(a.geom,b.geom,10) \\\n",
    "        AND a.id <> b.id AND a.year <> b.year ORDER BY ST_X(a.geom),a.year;\"\n",
    "    }\n",
    "    return switcher.get(argument,\"Invalid choise - by myself\")\n",
    "\n",
    "# WHERE crop ILIKE '%vete%'  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grouping weather data by Seasons, Month, week - do a total "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total average of all columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# REMOVE?!\n",
    "\n",
    "# Defining the different possible calls to be used later\n",
    "def choose_group(argument):\n",
    "    switcher = {\n",
    "        # Gets blockids from cultivation - Heddinge only\n",
    "        \"D\": '''Nothin.... ''',\n",
    "        \n",
    "        # Gets all from soil - specific field only\n",
    "        \"W\": \"nothing\",\n",
    "        \n",
    "        # Gets all from cultivation - specific: Varvete, Heddinge and seed_date\n",
    "        \"M\": '''nothing ...'''\n",
    "\n",
    "    }\n",
    "    return switcher.get(argument,\"Invalid choise. \\\n",
    "    Please choose from 'D', 'W' or 'M' for day, week or month respectively \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cutoutseason(datafrin, seasonb, seasone):\n",
    "    \n",
    "    datafrout = datafrin[datafrin[\"w_time\"]  >seasonb]\n",
    "    datafrout = datafrout[datafrout[\"w_time\"]<seasone]\n",
    "    \n",
    "    return datafrout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Defining the different possible groupings to choose from \n",
    "# 0: group all weather data into a single group\n",
    "# 1: group all weather data per week\n",
    "# 2: group all weather data per month\n",
    "# 3: group all weather data per specific dates\n",
    "\n",
    "def group_weather(argument, tempdf, fieldf, year_chosen):#, fieldID = 123456789):\n",
    "    \n",
    "    if argument == 0:   # average all data into a single group (as an average)\n",
    "        ntempdf = pd.DataFrame(columns=tempdf.columns)\n",
    "        ntempdf.loc['mean'] = tempdf.mean() # do an average over the total\n",
    "        \n",
    "    elif argument == 1: # group weather data per week (as an average)\n",
    "        \n",
    "        format_string = \"%Y-%m-%d\"\n",
    "        tempdf[\"w_time\"] = pd.to_datetime(tempdf[\"w_time\"], format = format_string)\n",
    "\n",
    "        # choose to keep year 2020\n",
    "        tempdf = elim(tempdf, \"w_year\", year_chosen)\n",
    "        \n",
    "        groupby = \"W\"  # group by week, (M) Month. Alternatively \"D\" for day\n",
    "        ntempdf = tempdf.groupby(pd.Grouper(key='w_time',freq = groupby)).mean()\n",
    "        \n",
    "    elif argument == 2: # group weather data per month (as an average)\n",
    "        # First change the date column to datetime object\n",
    "        # Convert date and time to just datetime object with only date\n",
    "        format_string = \"%Y-%m-%d\"\n",
    "        tempdf[\"w_time\"] = pd.to_datetime(tempdf[\"w_time\"], format = format_string)\n",
    "\n",
    "        # choose to keep year 2020\n",
    "        tempdf = elim(tempdf, \"w_year\", year_chosen)\n",
    "\n",
    "        groupby = \"M\"  # group by week, (M) Month. Alternatively \"D\" for day\n",
    "        ntempdf = tempdf.groupby(pd.Grouper(key='w_time',freq = groupby)).mean()\n",
    "        \n",
    "    elif argument == 3: # average over specific seasons \n",
    "        # First create a format for indexing seed date\n",
    "        #format_string = \"%Y-%m-%d\"\n",
    "        #nfieldf[\"w_time\"] = pd.to_datetime(nfieldf[\"f_seed_date\"], format = format_string)\n",
    "        #fieldf[\"f_harvest_date\"] = pd.to_datetime(fieldf[\"f_harvest_date\"], format = format_string)\n",
    "        #print(\"Full field dataset was \", nfieldf.shape)\n",
    "        #print(\"Number of fields seeded in \", year_chosen-1,\" were: \", \\\n",
    "        #      len(nfieldf[nfieldf[\"f_seed_date\"]<datetime.date(year_chosen-1,10,30)]) )\n",
    "\n",
    "        print(\"The following will be used as a start time for weather grouping.\")\n",
    "        # Find minimum date of seeding\n",
    "        min_seed_date = min(fieldf[fieldf[\"f_seed_date\"]>datetime.date(year_chosen-1,9,2)][\"f_seed_date\"])#.shape\n",
    "        print(\"The earliest seeding date is \", min_seed_date)\n",
    "\n",
    "        # Get harvest date for this dataset\n",
    "        max_harv_date = max(fieldf[fieldf[\"f_harvest_date\"]<datetime.date(year_chosen,9,1)][\"f_harvest_date\"])\n",
    "        print(\"MAximum harvest time: \", max_harv_date)\n",
    "        #max_harv_date = max(fieldf[fieldf[\"f_harvest_date\"]>datetime.date(year_chosen,7,1)][\"f_seed_date\"])\n",
    "\n",
    "        # Season 1: Seed-start-> 1st Nov\n",
    "        season1st  = min_seed_date\n",
    "        season1end = datetime.date(year_chosen-1,11,1) # November 1st\n",
    "        # Season 2: 15 March  -> Midsummer\n",
    "        season2st  = datetime.date(year_chosen,3,15)   # March 15\n",
    "        season2end = datetime.date(year_chosen,6,25)   # June 25 (midsummer)\n",
    "        # Season 3: Midsummer -> Harvest date\n",
    "        season3st  = datetime.date(year_chosen,6,26)   # June 26 (after midsummer)\n",
    "        season3end = max_harv_date\n",
    "        \n",
    "        print(\"Dates for season 1: \", season1st, season1end)\n",
    "        print(\"Dates for season 2: \", season2st, season2end)\n",
    "        print(\"Dates for season 3: \", season3st, season3end)\n",
    "\n",
    "        t1 = cutoutseason(tempdf, season1st, season1end)\n",
    "        if len(t1) == 0:\n",
    "            print(\"Problems!!!\")\n",
    "            t1\n",
    "            \n",
    "        t1[\"mean\"]=t1.mean()\n",
    "        t2 = cutoutseason(tempdf, season2st, season2end)\n",
    "        t3 = cutoutseason(tempdf, season3st, season3end)\n",
    "        #ntempdf = pd.DataFrame(columns=tempdf.columns)\n",
    "        #t1 = tempdf[tempdf[\"f_harvest_date\"]]>season1st\n",
    "        #t2 = t1[t1[\"f_harvest_date\"]]<season1end\n",
    "        ntempdf = pd.DataFrame()#columns=[\"w_precipitation\"])\n",
    "        l = pd.Series([t1.mean()[\"w_precipitation\"], t2.mean()[\"w_precipitation\"], t3.mean()[\"w_precipitation\"]])\n",
    "        ntempdf[\"w_precipitation\"] = l\n",
    "        #ntempdf # this is weather grouped by season\n",
    "        \n",
    "    \n",
    "    return ntempdf"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Now work on spatial proximity grouping between datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First with Harvest: for a given square with side length 2r we group together (sum) all harvest data which are within that distance for each coordinate in the soil data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of x and y coordinates from dataframes. \n",
    "def createdftoxylist1(nnharvdf, nnsoildf):\n",
    "    \n",
    "    xsoil_coord = list(nnsoildf[\"coord_x\"])   # x Soil coordinates \n",
    "    ysoil_coord = list(nnsoildf[\"coord_y\"])   # y Soil coordinates \n",
    "\n",
    "    xharv_coord = list(nnharvdf[\"h_coord_x\"]) # x Harvest coordinates\n",
    "    yharv_coord = list(nnharvdf[\"h_coord_y\"]) # y Harvest coordinates\n",
    "    \n",
    "    return xharv_coord, yharv_coord, xsoil_coord, ysoil_coord\n",
    "\n",
    "def remdata(nnharvdf, nnsoildf, r=10):\n",
    "    new_soil_df = nnsoildf.sort_values(\"coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    nnharvdf = nnharvdf.sort_values(\"h_coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    xharv_coord, yharv_coord, xsoil_coord, ysoil_coord = createdftoxylist(nnharvdf, new_soil_df)\n",
    "    new_harv_df = pd.DataFrame(columns=nnharvdf.keys())             # empty dataframe\n",
    "    \n",
    "    remind = [] # index of rows from harvest to be removed at end\n",
    "    \n",
    "    # HARVEST first\n",
    "    for s in tqdm(range(len(xharv_coord))): # Looping through all harvest coordinates\n",
    "        x_center = xharv_coord[s]\n",
    "        y_center = yharv_coord[s]\n",
    "\n",
    "        xlow = x_center-r \n",
    "        xhigh= x_center+r\n",
    "        ylow = y_center-r\n",
    "        yhigh= y_center+r    \n",
    "        cnt3=0\n",
    "        cnt2=0\n",
    "        \n",
    "        # SOIL next\n",
    "        for indx in range(len(xsoil_coord)):# Looping through all soil coordinates\n",
    "            sumind = [] # index of rows from soil to add-up later at end\n",
    "            xh = xharv_coord[indx]  # x soil coordinate\n",
    "            \n",
    "            if xh>xlow: # if within r of the x harv coordinate  then look at the y coordinate as well\n",
    "                yh = yharv_coord[indx]  # y soil coordinate\n",
    "                if yh>ylow and yh<yhigh: # we keep this index!\n",
    "                    sumind.append(indx) # indexes to sum up in soil\n",
    "                    cnt3 = cnt3+1\n",
    "                    #print(\"Soil\", xh, \"inside circle centered at harvest:\", x_center)\n",
    "            if xh>xhigh:\n",
    "                break # we are finished with this harv coordinate- move on to next soil coordinate\n",
    "                \n",
    "        if len(sumind) == 0: # i.e. no points within range then remove that index from soil\n",
    "            remind.append(indx) # indexes to remove from harvest\n",
    "        else:\n",
    "            new_soil_df.loc[cnt2]=list(nnsoildf.iloc[sumind,:].sum()/cnt3)  # average soil indexes\n",
    "            cnt2 = cnt2+1\n",
    "            \n",
    "    new_harv_df=nnharvdf.drop(remind)             # remove harvest indexes\n",
    "    \n",
    "    return new_harv_df, new_soil_df\n",
    "\n",
    "# For each point in the soil find all points in harvest within given radius\n",
    "# Once points found then we average all the soil contributions (within circle) into input vector dataframe grouped around those points\n",
    "# It creates a new harvest dataframe with same number of rows as those from soil dataframe\n",
    "def groupSoil(nnharvdf, nnsoildf, r=10):\n",
    "    #r = 10 # Radius is ad-hoc and given by user\n",
    "    \n",
    "    xharv_coord, yharv_coord, xsoil_coord, ysoil_coord = createdftoxylist(nnharvdf, nnsoildf)\n",
    "    \n",
    "    cnt2 = 0\n",
    "    for s in tqdm(range(len(xsoil_coord))): # Looping through all soil coordinates\n",
    "        print(\"at soil coordinate: \",s)\n",
    "        x_center = xsoil_coord[s]#geometry.Point(s)#[0],s[1])  # Note: center is given by soil coordinates!\n",
    "        y_center = ysoil_coord[s]\n",
    "\n",
    "        cnt3 = 0\n",
    "        rowind = [] # index of rows in harvest dataframe to be added together later on\n",
    "        xlow = x_center-r \n",
    "        xhigh= x_center+r\n",
    "        ylow = y_center-r\n",
    "        yhigh= y_center+r\n",
    "\n",
    "        for indx in range(len(xharv_coord)):#harv_coord: # Looping through all harvest coordinates\n",
    "            xh = xharv_coord[indx]\n",
    "\n",
    "            if xh>xlow: # if within r of the x soil coordinate  then look at the y coordinate as well\n",
    "                yh = yharv_coord[indx]\n",
    "                if yh>ylow and yh<yhigh: # we keep this index!\n",
    "                    rowind.append(indx)\n",
    "                    cnt3 = cnt3+1\n",
    "                    #print(\"Harvest\", xh, \"inside circle centered at soil:\", x_center)\n",
    "            if xh>xhigh:\n",
    "                break # we are finished with this soil coordinate- move on to next soil coordinate\n",
    "\n",
    "        #print(\"Added \", cnt3, \" harvest data within this soil center.\" )\n",
    "        if rowind == []:\n",
    "            print(\"             PROBLE<S!!     No harvest data near this soil coordinate \")\n",
    "        newdf.loc[cnt2]=list(newharvdf.iloc[rowind,:].sum()/cnt3)  # average harvest\n",
    "        #print(\"item included is index:\",cnt2,\" with value \", nnharvdf.iloc[rowind,:].sum()/cnt3)\n",
    "        cnt2 = cnt2+1\n",
    "        \n",
    "    return newdf # Harvest DataFrame grouped according to Soil coordinates (same number of row - in same order)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of x and y coordinates from dataframes. \n",
    "def h_a_h_createdftoxylist(nnharvdf, nnsoildf):\n",
    "    \n",
    "    xsoil_coord = list(nnsoildf[\"h_coord_x\"])   # x Soil coordinates \n",
    "    ysoil_coord = list(nnsoildf[\"h_coord_y\"])   # y Soil coordinates \n",
    "\n",
    "    xharv_coord = list(nnharvdf[\"h_coord_x\"]) # x Harvest coordinates\n",
    "    yharv_coord = list(nnharvdf[\"h_coord_y\"]) # y Harvest coordinates\n",
    "    \n",
    "    return xharv_coord, yharv_coord, xsoil_coord, ysoil_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_around_harvest(nnharvdf, nnsoildf, r=10):\n",
    "    new_soil_df = nnsoildf.sort_values(\"h_coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    nnharvdf = nnharvdf.sort_values(\"h_coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    xharv_coord, yharv_coord, xsoil_coord, ysoil_coord = h_a_h_createdftoxylist(nnharvdf, new_soil_df)\n",
    "    new_harv_df = pd.DataFrame(columns=nnharvdf.keys())             # empty dataframe\n",
    "    \n",
    "    remind = [] # index of rows from harvest to be removed at end\n",
    "    cnt2=0\n",
    "    \n",
    "    # SOIL first\n",
    "    for xs in tqdm(range(len(xsoil_coord))): # Looping through all soil coordinates\n",
    "        x_center = xsoil_coord[xs]\n",
    "        y_center = ysoil_coord[xs]\n",
    "\n",
    "        xlow = x_center-r \n",
    "        xhigh= x_center+r\n",
    "        ylow = y_center-r\n",
    "        yhigh= y_center+r    \n",
    "        cnt3=0\n",
    "\n",
    "        sumind = [] # index of rows from soil to add-up later at end\n",
    "        \n",
    "        # HARVEST next\n",
    "        for xind in range(len(xharv_coord)):# Looping through all soil coordinates\n",
    "            \n",
    "            xh = xharv_coord[xind]  # x soil coordinate\n",
    "            \n",
    "            if xh>xlow: # if within r of the x harv coordinate  then look at the y coordinate as well\n",
    "                yh = yharv_coord[xind]  # y soil coordinate\n",
    "                if yh>ylow and yh<yhigh: # we keep this index!\n",
    "                    sumind.append(xind) # indexes to sum up in soil\n",
    "                    cnt3 = cnt3+1\n",
    "                    #print(\"Soil\", xh, \"inside circle centered at harvest:\", x_center)\n",
    "            if xh>xhigh:\n",
    "                break # we are finished with this harv coordinate- move on to next soil coordinate\n",
    "                \n",
    "        if len(sumind) == 0: # i.e. no points within range then remove that index from soil\n",
    "            remind.append(xs) # indexes to remove from soil\n",
    "            cn2 = cnt2+1\n",
    "        else:\n",
    "            new_harv_df.loc[cnt2]=list(nnharvdf.iloc[sumind,:].sum()/cnt3)  # average harv indexes\n",
    "            cnt2 = cnt2+1\n",
    "    df = new_soil_df\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(df.index[remind], inplace=True)\n",
    "    new_soil_df=df #new_soil_df.drop(remind)             # remove harvest indexes\n",
    "    #print(\"Soil shape\", new_soil_df.shape)\n",
    "    print(\"New Harvest shape\", new_harv_df.shape)\n",
    "    return new_harv_df #, new_soil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of x and y coordinates from dataframes. \n",
    "def s_a_h_createdftoxylist(nnharvdf, nnsoildf):\n",
    "    \n",
    "    xsoil_coord = list(nnsoildf[\"h_coord_x\"])   # x Soil coordinates \n",
    "    ysoil_coord = list(nnsoildf[\"h_coord_y\"])   # y Soil coordinates \n",
    "\n",
    "    xharv_coord = list(nnharvdf[\"coord_x\"]) # x Harvest coordinates\n",
    "    yharv_coord = list(nnharvdf[\"coord_y\"]) # y Harvest coordinates\n",
    "    \n",
    "    return xharv_coord, yharv_coord, xsoil_coord, ysoil_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def soil_around_harvest(nnharvdf, nnsoildf, r=10):\n",
    "    new_soil_df = nnsoildf.sort_values(\"h_coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    nnharvdf = nnharvdf.sort_values(\"coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    xharv_coord, yharv_coord, xsoil_coord, ysoil_coord = s_a_h_createdftoxylist(nnharvdf, new_soil_df)\n",
    "    new_harv_df = pd.DataFrame(columns=nnharvdf.keys())             # empty dataframe\n",
    "    \n",
    "    remind = [] # index of rows from harvest to be removed at end\n",
    "    cnt2=0\n",
    "    \n",
    "    # SOIL first\n",
    "    for xs in tqdm(range(len(xsoil_coord))): # Looping through all soil coordinates\n",
    "        x_center = xsoil_coord[xs]\n",
    "        y_center = ysoil_coord[xs]\n",
    "\n",
    "        xlow = x_center-r \n",
    "        xhigh= x_center+r\n",
    "        ylow = y_center-r\n",
    "        yhigh= y_center+r    \n",
    "        cnt3=0\n",
    "\n",
    "        sumind = [] # index of rows from soil to add-up later at end\n",
    "        \n",
    "        # HARVEST next\n",
    "        for xind in range(len(xharv_coord)):# Looping through all soil coordinates\n",
    "            \n",
    "            xh = xharv_coord[xind]  # x soil coordinate\n",
    "            \n",
    "            if xh>xlow: # if within r of the x harv coordinate  then look at the y coordinate as well\n",
    "                yh = yharv_coord[xind]  # y soil coordinate\n",
    "                if yh>ylow and yh<yhigh: # we keep this index!\n",
    "                    sumind.append(xind) # indexes to sum up in soil\n",
    "                    cnt3 = cnt3+1\n",
    "                    #print(\"Soil\", xh, \"inside circle centered at harvest:\", x_center)\n",
    "            if xh>xhigh:\n",
    "                break # we are finished with this harv coordinate- move on to next soil coordinate\n",
    "                \n",
    "        if len(sumind) == 0: # i.e. no points within range then remove that index from soil\n",
    "            remind.append(xs) # indexes to remove from soil\n",
    "            cn2 = cnt2+1\n",
    "        else:\n",
    "            new_harv_df.loc[cnt2]=list(nnharvdf.iloc[sumind,:].sum()/cnt3)  # average harv indexes\n",
    "            cnt2 = cnt2+1\n",
    "    df = new_soil_df\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(df.index[remind], inplace=True)\n",
    "    new_soil_df=df #new_soil_df.drop(remind)             # remove harvest indexes\n",
    "    print(\"Soil shape\", new_soil_df.shape)\n",
    "    print(\"Harv shape\", new_harv_df.shape)\n",
    "    return new_harv_df, new_soil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creates lists of x and y coordinates from dataframes. \n",
    "def h_a_s_createdftoxylist(nnharvdf, nnsoildf):\n",
    "    \n",
    "    xsoil_coord = list(nnsoildf[\"coord_x\"])   # x Soil coordinates \n",
    "    ysoil_coord = list(nnsoildf[\"coord_y\"])   # y Soil coordinates \n",
    "\n",
    "    xharv_coord = list(nnharvdf[\"h_coord_x\"]) # x Harvest coordinates\n",
    "    yharv_coord = list(nnharvdf[\"h_coord_y\"]) # y Harvest coordinates\n",
    "    \n",
    "    return xharv_coord, yharv_coord, xsoil_coord, ysoil_coord"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "def harvest_around_soil(nnharvdf, nnsoildf, r=10):\n",
    "    new_soil_df = nnsoildf.sort_values(\"coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    nnharvdf = nnharvdf.sort_values(\"h_coord_x\") # sort soil dataframe based on x_coordinate\n",
    "    xharv_coord, yharv_coord, xsoil_coord, ysoil_coord = h_a_s_createdftoxylist(nnharvdf, new_soil_df)\n",
    "    new_harv_df = pd.DataFrame(columns=nnharvdf.keys())             # empty dataframe\n",
    "    \n",
    "    remind = [] # index of rows from harvest to be removed at end\n",
    "    cnt2=0\n",
    "    \n",
    "    # SOIL first\n",
    "    for xs in tqdm(range(len(xsoil_coord))): # Looping through all soil coordinates\n",
    "        x_center = xsoil_coord[xs]\n",
    "        y_center = ysoil_coord[xs]\n",
    "\n",
    "        xlow = x_center-r \n",
    "        xhigh= x_center+r\n",
    "        ylow = y_center-r\n",
    "        yhigh= y_center+r    \n",
    "        cnt3=0\n",
    "\n",
    "        sumind = [] # index of rows from soil to add-up later at end\n",
    "        \n",
    "        # HARVEST next\n",
    "        for xind in range(len(xharv_coord)):# Looping through all soil coordinates\n",
    "            \n",
    "            xh = xharv_coord[xind]  # x soil coordinate\n",
    "            \n",
    "            if xh>xlow: # if within r of the x harv coordinate  then look at the y coordinate as well\n",
    "                yh = yharv_coord[xind]  # y soil coordinate\n",
    "                if yh>ylow and yh<yhigh: # we keep this index!\n",
    "                    sumind.append(xind) # indexes to sum up in soil\n",
    "                    cnt3 = cnt3+1\n",
    "                    #print(\"Soil\", xh, \"inside circle centered at harvest:\", x_center)\n",
    "            if xh>xhigh:\n",
    "                break # we are finished with this harv coordinate- move on to next soil coordinate\n",
    "                \n",
    "        if len(sumind) == 0: # i.e. no points within range then remove that index from soil\n",
    "            remind.append(xs) # indexes to remove from soil\n",
    "            cn2 = cnt2+1\n",
    "        else:\n",
    "            new_harv_df.loc[cnt2]=list(nnharvdf.iloc[sumind,:].sum()/cnt3)  # average harv indexes\n",
    "            cnt2 = cnt2+1\n",
    "    df = new_soil_df\n",
    "    df.reset_index(drop=True, inplace=True)\n",
    "    df.drop(df.index[remind], inplace=True)\n",
    "    new_soil_df=df #new_soil_df.drop(remind)             # remove harvest indexes\n",
    "    print(\"Soil shape\", new_soil_df.shape)\n",
    "    print(\"Harv shape\", new_harv_df.shape)\n",
    "    return new_harv_df, new_soil_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation Functions\n",
    "def rmse(model, y_test, y_pred, X_train, y_train):\n",
    " r_squared = model.score(X_test, y_test)\n",
    " mse = mean_squared_error(y_test, y_pred)\n",
    " rmse = np.sqrt(mse)\n",
    " print('R-squared: ' + str(r_squared))\n",
    " print('Mean Squared Error: '+ str(rmse))\n",
    "\n",
    "# Create model line scatter plot\n",
    "def scatter_plot(y_test, y_pred, model_name):\n",
    "    plt.figure(figsize=(10,6))\n",
    "    sns.residplot(y_test, y_pred, lowess=True, color='#4682b4',\n",
    "              line_kws={'lw': 2, 'color': 'r'})\n",
    "    plt.title(str('Price vs Residuals for '+ model_name))\n",
    "    plt.xlabel('Price',fontsize=16)\n",
    "    plt.xticks(fontsize=13)\n",
    "    plt.yticks(fontsize=13)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def heatmap(x, y, **kwargs):\n",
    "    if 'color' in kwargs:\n",
    "        color = kwargs['color']\n",
    "    else:\n",
    "        color = [1]*len(x)\n",
    "\n",
    "    if 'palette' in kwargs:\n",
    "        palette = kwargs['palette']\n",
    "        n_colors = len(palette)\n",
    "    else:\n",
    "        n_colors = 256 # Use 256 colors for the diverging color palette\n",
    "        palette = sns.color_palette(\"Blues\", n_colors) \n",
    "\n",
    "    if 'color_range' in kwargs:\n",
    "        color_min, color_max = kwargs['color_range']\n",
    "    else:\n",
    "        color_min, color_max = min(color), max(color) # Range of values that will be mapped to the palette, i.e. min and max possible correlation\n",
    "\n",
    "    def value_to_color(val):\n",
    "        if color_min == color_max:\n",
    "            return palette[-1]\n",
    "        else:\n",
    "            val_position = float((val - color_min)) / (color_max - color_min) # position of value in the input range, relative to the length of the input range\n",
    "            val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n",
    "            ind = int(val_position * (n_colors - 1)) # target index in the color palette\n",
    "            return palette[ind]\n",
    "\n",
    "    if 'size' in kwargs:\n",
    "        size = kwargs['size']\n",
    "    else:\n",
    "        size = [1]*len(x)\n",
    "\n",
    "    if 'size_range' in kwargs:\n",
    "        size_min, size_max = kwargs['size_range'][0], kwargs['size_range'][1]\n",
    "    else:\n",
    "        size_min, size_max = min(size), max(size)\n",
    "\n",
    "    size_scale = kwargs.get('size_scale', 500)\n",
    "\n",
    "    def value_to_size(val):\n",
    "        if size_min == size_max:\n",
    "            return 1 * size_scale\n",
    "        else:\n",
    "            val_position = (val - size_min) * 0.99 / (size_max - size_min) + 0.01 # position of value in the input range, relative to the length of the input range\n",
    "            val_position = min(max(val_position, 0), 1) # bound the position betwen 0 and 1\n",
    "            return val_position * size_scale\n",
    "    if 'x_order' in kwargs: \n",
    "        x_names = [t for t in kwargs['x_order']]\n",
    "    else:\n",
    "        x_names = [t for t in sorted(set([v for v in x]))]\n",
    "    x_to_num = {p[1]:p[0] for p in enumerate(x_names)}\n",
    "\n",
    "    if 'y_order' in kwargs: \n",
    "        y_names = [t for t in kwargs['y_order']]\n",
    "    else:\n",
    "        y_names = [t for t in sorted(set([v for v in y]))]\n",
    "    y_to_num = {p[1]:p[0] for p in enumerate(y_names)}\n",
    "\n",
    "    plot_grid = plt.GridSpec(1, 15, hspace=0.2, wspace=0.1) # Setup a 1x10 grid\n",
    "    ax = plt.subplot(plot_grid[:,:-1]) # Use the left 14/15ths of the grid for the main plot\n",
    "\n",
    "    marker = kwargs.get('marker', 's')\n",
    "\n",
    "    kwargs_pass_on = {k:v for k,v in kwargs.items() if k not in [\n",
    "         'color', 'palette', 'color_range', 'size', 'size_range', 'size_scale', 'marker', 'x_order', 'y_order'\n",
    "    ]}\n",
    "\n",
    "    ax.scatter(\n",
    "        x=[x_to_num[v] for v in x],\n",
    "        y=[y_to_num[v] for v in y],\n",
    "        marker=marker,\n",
    "        s=[value_to_size(v) for v in size], \n",
    "        c=[value_to_color(v) for v in color],\n",
    "        **kwargs_pass_on\n",
    "    )\n",
    "    ax.set_xticks([v for k,v in x_to_num.items()])\n",
    "    ax.set_xticklabels([k for k in x_to_num], rotation=45, horizontalalignment='right')\n",
    "    ax.set_yticks([v for k,v in y_to_num.items()])\n",
    "    ax.set_yticklabels([k for k in y_to_num])\n",
    "\n",
    "    ax.grid(False, 'major')\n",
    "    ax.grid(True, 'minor')\n",
    "    ax.set_xticks([t + 0.5 for t in ax.get_xticks()], minor=True)\n",
    "    ax.set_yticks([t + 0.5 for t in ax.get_yticks()], minor=True)\n",
    "\n",
    "    ax.set_xlim([-0.5, max([v for v in x_to_num.values()]) + 0.5])\n",
    "    ax.set_ylim([-0.5, max([v for v in y_to_num.values()]) + 0.5])\n",
    "    ax.set_facecolor('#F1F1F1')\n",
    "\n",
    "    # Add color legend on the right side of the plot\n",
    "    if color_min < color_max:\n",
    "        ax = plt.subplot(plot_grid[:,-1]) # Use the rightmost column of the plot\n",
    "\n",
    "        col_x = [0]*len(palette) # Fixed x coordinate for the bars\n",
    "        bar_y=np.linspace(color_min, color_max, n_colors) # y coordinates for each of the n_colors bars\n",
    "\n",
    "        bar_height = bar_y[1] - bar_y[0]\n",
    "        ax.barh(\n",
    "            y=bar_y,\n",
    "            width=[5]*len(palette), # Make bars 5 units wide\n",
    "            left=col_x, # Make bars start at 0\n",
    "            height=bar_height,\n",
    "            color=palette,\n",
    "            linewidth=0\n",
    "        )\n",
    "        ax.set_xlim(1, 2) # Bars are going from 0 to 5, so lets crop the plot somewhere in the middle\n",
    "        ax.grid(False) # Hide grid\n",
    "        ax.set_facecolor('white') # Make background white\n",
    "        ax.set_xticks([]) # Remove horizontal ticks\n",
    "        ax.set_yticks(np.linspace(min(bar_y), max(bar_y), 3)) # Show vertical ticks for min, middle and max\n",
    "        ax.yaxis.tick_right() # Show vertical ticks on the right \n",
    "\n",
    "\n",
    "def corrplot(data, size_scale=500, marker='s'):\n",
    "    corr = pd.melt(data.reset_index(), id_vars='index')\n",
    "    corr.columns = ['x', 'y', 'value']\n",
    "    heatmap(\n",
    "        corr['x'], corr['y'],\n",
    "        color=corr['value'], color_range=[-1, 1],\n",
    "        palette=sns.diverging_palette(20, 220, n=256),\n",
    "        size=corr['value'].abs(), size_range=[0,1],\n",
    "        marker=marker,\n",
    "        x_order=data.columns,\n",
    "        y_order=data.columns[::-1],\n",
    "        size_scale=size_scale\n",
    "    )\n"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "createHarvestDFfrompickleJuly9.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
